{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d57c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d7198",
   "metadata": {},
   "source": [
    "# From Shallow to Deep Neural Networks\n",
    "\n",
    "We will explore how the depth of a network interacts with its trainability and performance. \n",
    "\n",
    "In the previous assignment you likely observed difficulties in training sigmoid and ReLU networks with over ~8 layers, which is typically associated with 'vanishing' or 'exploding' gradients. As you will see, some of the biggest achievements in deep learning have been the development of techniques that enable deeper networks to be successfully trained, and without them deep networks are notoriously difficult to train successfully.\n",
    "\n",
    "You will be working with the MNIST dataset, which will be downloaded and loaded in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8407442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
